# Data-Driven-Polymer-Property-Prediction-with-Machine-Learning
This repository contains my solution for the [NeurIPS 2025 Open Polymer Prediction](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025) Kaggle competition.   The goal is to **predict multiple polymer properties** (e.g., Tg, FFV, Density, Rg, Tc) from SMILES chemical representations.
---

## ğŸ”¹ Project Highlights

- Built an **end-to-end ML pipeline** for multi-target regression with incomplete and noisy data.
- Engineered **molecular descriptors & fingerprints** (Morgan fingerprints, RDKit descriptors).
- Designed robust **feature preprocessing & selection**:
  - Dropped NaN-heavy columns
  - Variance and correlation filtering
  - Supervised feature selection
- Developed a **target imputation module** using Neural Networks and iterative imputation to handle >50% missing labels for some targets.
- Trained and ensembled **LightGBM, XGBoost** models with K-fold cross-validation.
- Achieved a **0.083 MAE leaderboard score** (~1200th place out of thousands globally).

---

## ğŸ“‚ Directory Structure

```
./
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ configs/
â”‚  â”œâ”€ config.yaml                  # paths, feature flags, selection top_k, thresholds
â”‚  â”œâ”€ tuned_xgb.json               # your per-target tuned XGB params (from Optuna)
â”œâ”€ data/
â”‚  â”œâ”€ raw/                         # train.csv, test.csv, supplement files
â”‚  â”œâ”€ interim/                     # canonical csvs, merged extras
â”‚  â””â”€ processed/                   # features (parquet/npy), masks, state pickles
â”œâ”€ notebooks/
â”‚  â””â”€ polymer-lgbm-xgb.ipynb       # your original notebook (archived)
â”œâ”€ scripts/
â”‚  â”œâ”€ make_features.py             # build RDKit descriptors (+ optional graph feats)
â”‚  â”œâ”€ train_xgb.py                 # KFold training using tuned per-target XGB
â”‚  â”œâ”€ predict.py                   # load fold models and create submission.csv
â”‚  â””â”€ tune_xgb.py                  # Optuna tuning per target (optional)
â””â”€ src/
   â”œâ”€ __init__.py
   â”œâ”€ data/
   â”‚  â”œâ”€ __init__.py
   â”‚  â”œâ”€ io.py                     # load/validate CSVs, canonicalize smiles
   â”‚  â””â”€ augment.py                # merge extra datasets into train
   â”œâ”€ featurization/
   â”‚  â”œâ”€ __init__.py
   â”‚  â”œâ”€ rdkit_feats.py            # descriptors, Morgan (optional), graph features
   â”‚  â””â”€ graphs.py                 # graph_diameter, avg_shortest_path, num_cycles
   â”œâ”€ preprocessing/
   â”‚  â”œâ”€ __init__.py
   â”‚  â”œâ”€ preprocessor.py           # fit/transform (variance/corr/scale) + state
   â”‚  â”œâ”€ selection.py              # per_target_supervised_selection
   â”‚  â””â”€ imputation.py             # impute_targets_adaptive (NN/simple)
   â”œâ”€ models/
   â”‚  â”œâ”€ __init__.py
   â”‚  â”œâ”€ tuned_params.py           # loads configs/tuned_xgb.json
   â”‚  â”œâ”€ xgb.py                    # make_xgb_model, kfold_train_xgb
   â”‚  â””â”€ infer.py                  # predict_and_make_submission_xgb (and generic)
   â”œâ”€ tuning/
   â”‚  â”œâ”€ tuning_xgb.py             # tune_xgb_mae
   â”œâ”€ viz/
   â”‚  â””â”€ target_dist.py                  # plot_corr, target_corr, distributions, overlays
   â””â”€ utils/
      â”œâ”€ __init__.py
      â”œâ”€ metrics.py
      â”œâ”€ logging.py
      â””â”€ seed.py
```

---

## ğŸš€ How to Run

1. **Clone the repository:**
   ```bash
   git clone https://github.com/<your-username>/polymer-prediction.git
   cd polymer-prediction
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Download Kaggle competition data:**
   ```bash
   kaggle competitions download -c neurips-open-polymer-prediction-2025 -p data/raw
   unzip data/raw/*.zip -d data/raw
   ```

4. **Run preprocessing and training:**
   ```bash
   python src/data_preprocessing.py
   python src/models.py
   ```

5. **Generate a submission file:**
   ```bash
   python src/models.py --predict-test --output submissions/submission.csv
   ```

---

## ğŸ“¦ Requirements

- Python 3.9+
- Pandas, NumPy, scikit-learn
- LightGBM, XGBoost
- RDKit (for chemical feature engineering)
- PyTorch / TensorFlow (for NN imputation)

See [requirements.txt](requirements.txt) for the full list.
## Quickstart
```bash
pip install -r requirements.txt

# edit configs/config.yaml paths (train/test/supplements)
python scripts/make_features.py --config configs/config.yaml
python scripts/train_xgb.py --config configs/config.yaml
python scripts/predict.py --config configs/config.yaml -o submission.csv
---

## ğŸ† Kaggle Results

- **Leaderboard Score:** 0.083 (multi-target MAE)
- **Ranking:** ~1200 out of thousands globally

---

## ğŸ”— References

- [Kaggle competition page](https://www.kaggle.com/competitions/neurips-open-polymer-prediction-2025)
- [RDKit: Open-source cheminformatics](https://www.rdkit.org/)
