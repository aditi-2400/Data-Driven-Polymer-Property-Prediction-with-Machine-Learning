{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --no-index \\\n  --find-links=/kaggle/input/rdkit-2025-3-3-cp311 \\\n  /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:26:44.257802Z","iopub.execute_input":"2025-07-30T06:26:44.258065Z","iopub.status.idle":"2025-07-30T06:26:50.279676Z","shell.execute_reply.started":"2025-07-30T06:26:44.258041Z","shell.execute_reply":"2025-07-30T06:26:50.278989Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors, AllChem\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.experimental import enable_iterative_imputer  # noqa: F401\nfrom sklearn.impute import IterativeImputer\nfrom rdkit import RDLogger\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import KFold\nRDLogger.DisableLog('rdApp.*')\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nRDLogger.DisableLog('rdApp.*')\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.preprocessing import StandardScaler\n_HAS_LGBM = _HAS_XGB = False\ntry:\n    from lightgbm import LGBMRegressor\n    _HAS_LGBM = True\nexcept Exception:\n    pass\n\ntry:\n    from xgboost import XGBRegressor\n    _HAS_XGB = True\nexcept Exception:\n    pass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T07:31:03.617495Z","iopub.execute_input":"2025-07-30T07:31:03.617789Z","iopub.status.idle":"2025-07-30T07:31:07.284342Z","shell.execute_reply.started":"2025-07-30T07:31:03.617769Z","shell.execute_reply":"2025-07-30T07:31:07.283703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train.csv')\ntest = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\nTARGETS = ['Tg','FFV','Tc','Density','Rg']\ntrain.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:27:13.764185Z","iopub.execute_input":"2025-07-30T06:27:13.764684Z","iopub.status.idle":"2025-07-30T06:27:13.847428Z","shell.execute_reply.started":"2025-07-30T06:27:13.764655Z","shell.execute_reply":"2025-07-30T06:27:13.846669Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:27:18.864379Z","iopub.execute_input":"2025-07-30T06:27:18.864650Z","iopub.status.idle":"2025-07-30T06:27:18.871493Z","shell.execute_reply.started":"2025-07-30T06:27:18.864631Z","shell.execute_reply":"2025-07-30T06:27:18.870789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nan_counts = train.isna().sum()\nprint(nan_counts)\nprint(len(train))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:27:21.414000Z","iopub.execute_input":"2025-07-30T06:27:21.414285Z","iopub.status.idle":"2025-07-30T06:27:21.420643Z","shell.execute_reply.started":"2025-07-30T06:27:21.414266Z","shell.execute_reply":"2025-07-30T06:27:21.419778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Utility functions","metadata":{}},{"cell_type":"code","source":"def build_features(df, radius=2, nBits=512):\n    \"\"\"\n    Compute ALL RDKit descriptors + Morgan fingerprints for each SMILES.\n    Returns: pd.DataFrame (index == df.index), list(desc_names), list(fp_names)\n    \"\"\"\n    # All RDKit descriptors\n    desc_list = Descriptors._descList\n    desc_names = [d[0] for d in desc_list]\n    fp_names   = [f'FP_{i}' for i in range(nBits)]\n\n    rows = []\n    valid_idx = []\n    for idx, smi in zip(df.index, df['SMILES']):\n        try:\n            mol = Chem.MolFromSmiles(smi)\n            if mol is None:\n                # Skip invalid SMILES\n                print(f\"[Warning] Skipping invalid SMILES at index {idx}: {smi}\")\n                continue\n    \n            # Compute descriptors safely\n            dvals = []\n            for _, func in desc_list:\n                try:\n                    dvals.append(func(mol))\n                except Exception as e:\n                    dvals.append(np.nan)\n                    print(f\"[Warning] Descriptor failed for SMILES={smi}: {e}\")\n    \n            # Compute fingerprint safely\n            try:\n                fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n                # Convert RDKit ExplicitBitVect to numpy array\n                fp_arr = np.array(fp)\n                if fp_arr.ndim == 0:  # fallback if above fails\n                    fp_arr = np.frombuffer(fp.ToBitString().encode('utf-8'), 'S1').astype(int)\n            except Exception as e:\n                print(f\"[Warning] Fingerprint failed for SMILES={smi}: {e}\")\n                fp_arr = np.zeros(nBits, dtype=int)\n    \n            rows.append(dvals + fp_arr.tolist())\n            valid_idx.append(idx)\n    \n        except Exception as e:\n            print(f\"[Error] Failed to process SMILES at index {idx}: {smi}, Error: {e}\")\n            continue\n\n    try:\n        feat_df = pd.DataFrame(rows, index=valid_idx, columns=desc_names + fp_names)\n    except Exception as e:\n        print(f\"[Error] Could not create feature DataFrame: {e}\")\n        feat_df = pd.DataFrame(columns=desc_names + fp_names)\n    \n    return feat_df, desc_names, fp_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:27:24.446724Z","iopub.execute_input":"2025-07-30T06:27:24.447026Z","iopub.status.idle":"2025-07-30T06:27:24.455279Z","shell.execute_reply.started":"2025-07-30T06:27:24.446998Z","shell.execute_reply":"2025-07-30T06:27:24.454545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def fit_preprocessor(X_train: pd.DataFrame,\n                     variance_threshold: float = 0.0,\n                     corr_threshold: float = 0.90,\n                     scale: bool = True):\n    \"\"\"\n    Fit-time preprocessing (only on training data):\n    - Drop columns with NaNs\n    - Remove low-variance columns\n    - Remove highly correlated columns\n    - Fit scaler\n    Returns:\n        X_clean (np.ndarray),\n        state (dict with transformation state),\n        info (summary of dropped/kept columns)\n    \"\"\"\n    from sklearn.feature_selection import VarianceThreshold\n    from sklearn.preprocessing import StandardScaler\n\n    info = {}\n    try:\n        # Ensure string column names to avoid dtype/name mismatches later\n        X = X_train.copy()\n        X.columns = X.columns.astype(str)\n\n        # Step 1: Drop any column that has at least one NaN in TRAIN\n        X = X.replace([np.inf, -np.inf], np.nan)\n        nan_cols = X.columns[X.isna().any()].tolist()\n        info['dropped_nan_cols'] = nan_cols\n        X = X.drop(columns=nan_cols)\n\n        if X.empty:\n            raise ValueError(\"All columns dropped due to NaNs\")\n\n        # Step 2: Variance threshold (fit on TRAIN)\n        vt = VarianceThreshold(threshold=variance_threshold)\n        vt.fit(X)\n        kept_var_mask = vt.get_support()\n        vt_cols = X.columns[kept_var_mask].tolist()   # store these explicitly\n        dropped_var = [c for c in X.columns if c not in vt_cols]\n        info['dropped_lowvar_cols'] = dropped_var\n\n        # Manually select instead of calling vt.transform here (safer)\n        X = X[vt_cols]\n\n        # Step 3: Drop highly correlated features (TRAIN only)\n        corr = X.corr().abs()\n        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n        to_drop_corr = [col for col in upper.columns if any(upper[col] > corr_threshold)]\n        info['dropped_corr_cols'] = to_drop_corr\n\n        X = X.drop(columns=to_drop_corr)\n        kept_cols = X.columns.tolist()\n        info['retained_cols'] = kept_cols\n\n        # Step 4: Scale (fit on TRAIN)\n        scaler = None\n        if scale:\n            scaler = StandardScaler()\n            X_scaled = scaler.fit_transform(X)\n        else:\n            X_scaled = X.values\n\n        state = {\n            'variance_threshold': variance_threshold,\n            'corr_threshold': corr_threshold,\n            'nan_cols': nan_cols,               # columns dropped due to NaNs in TRAIN\n            'vt_cols': vt_cols,                 # columns kept after variance step\n            'to_drop_corr': set(to_drop_corr),  # correlated columns to drop\n            'kept_cols': kept_cols,             # final kept columns (after corr)\n            'scaler': scaler\n        }\n\n        return X_scaled, state, info\n\n    except Exception as e:\n        print(f\"[Error in fit_preprocessor] {e}\")\n        return np.array([]), {}, {'error': str(e)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:27:27.746521Z","iopub.execute_input":"2025-07-30T06:27:27.746798Z","iopub.status.idle":"2025-07-30T06:27:27.755475Z","shell.execute_reply.started":"2025-07-30T06:27:27.746777Z","shell.execute_reply":"2025-07-30T06:27:27.754601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def transform_preprocessor(X_df: pd.DataFrame, state):\n    \"\"\"\n    Apply the previously fitted preprocessor on new data (e.g., test).\n    Any missing columns are created as 0, extra columns are dropped.\n    Wrapped with try-except for safety in notebook submission.\n    \"\"\"\n    try:\n        X_df = X_df.copy()\n        X_df.columns = X_df.columns.astype(str)\n        X_df = X_df.replace([np.inf, -np.inf], np.nan)\n\n        # 1) drop the same NaN columns identified from TRAIN\n        drop_nan = [c for c in state.get('nan_cols', []) if c in X_df.columns]\n        X_df = X_df.drop(columns=drop_nan, errors='ignore')\n\n        # 2) ensure we have exactly the variance-kept columns (vt_cols), in order\n        vt_cols = list(state.get('vt_cols', []))\n        # add any missing vt_cols as zeros (safe default)\n        missing = [c for c in vt_cols if c not in X_df.columns]\n        if missing:\n            for c in missing:\n                X_df[c] = 0.0\n        # drop any extra columns not in vt_cols\n        X_df = X_df.reindex(columns=vt_cols, fill_value=0.0)\n\n        # 3) drop the correlated columns decided at TRAIN time\n        drop_corr = [c for c in state.get('to_drop_corr', []) if c in X_df.columns]\n        X_df = X_df.drop(columns=drop_corr, errors='ignore')\n\n        # 4) finally, ensure we have exactly the TRAIN-kept columns, in order\n        kept_cols = list(state.get('kept_cols', []))\n        missing_final = [c for c in kept_cols if c not in X_df.columns]\n        if missing_final:\n            for c in missing_final:\n                X_df[c] = 0.0\n        X_df = X_df.reindex(columns=kept_cols, fill_value=0.0)\n\n        # 5) scale if scaler exists\n        scaler = state.get('scaler', None)\n        if scaler is not None:\n            X_df = scaler.transform(X_df)\n\n        return X_df\n\n    except Exception as e:\n        print(f\"[ERROR] transform_preprocessor failed: {e}\")\n        # fallback to a zero array with the right number of final columns\n        n_rows = len(X_df)\n        n_cols = len(state.get('kept_cols', []))\n        return np.zeros((n_rows, n_cols))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:27:31.482807Z","iopub.execute_input":"2025-07-30T06:27:31.483100Z","iopub.status.idle":"2025-07-30T06:27:31.490519Z","shell.execute_reply.started":"2025-07-30T06:27:31.483080Z","shell.execute_reply":"2025-07-30T06:27:31.489763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def report_nan_columns_numpy(X: np.ndarray, name=\"X_train_clean\"):\n    try:\n        if X.ndim != 2:\n            print(f\"[WARN] {name} expected 2D array, got shape {X.shape}\")\n            return np.array([], dtype=int)\n        col_mask = np.isnan(X).any(axis=0)\n        idxs = np.where(col_mask)[0]\n        if idxs.size == 0:\n            print(f\"[{name}] No NaNs detected.\")\n        else:\n            counts = np.isnan(X).sum(axis=0)[idxs]\n            print(f\"[{name}] NaN columns (count={idxs.size}):\")\n            for j, c in zip(idxs, counts):\n                print(f\"  col {j} -> {c} NaNs\")\n        return idxs\n    except Exception as e:\n        print(f\"[WARN] report_nan_columns_numpy failed for {name}: {e}\")\n        return np.array([], dtype=int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:27:34.811862Z","iopub.execute_input":"2025-07-30T06:27:34.812638Z","iopub.status.idle":"2025-07-30T06:27:34.817665Z","shell.execute_reply.started":"2025-07-30T06:27:34.812612Z","shell.execute_reply":"2025-07-30T06:27:34.816901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def per_target_supervised_selection(\n    X_train_clean,          # np.ndarray or pd.DataFrame (after your preprocessing pipeline)\n    y_train,                # np.ndarray of shape (n_samples, n_targets) with NaNs\n    target_names,           # list of str\n    min_labels=400,         # below this, use lighter selection\n    base_estimator=None,    # pass a model; default RF\n    threshold=\"median\",     # SelectFromModel threshold\n    top_k=None,             # if set, override threshold and keep top-K by mean importance\n    cv_splits=5,\n    random_state=42,\n    verbose=True\n):\n    \"\"\"\n    Returns:\n      selectors: dict[target_name] -> fitted selector\n      Xsel_train: dict[target_name] -> selected X (train)\n    \"\"\"\n    if base_estimator is None:\n        base_estimator = RandomForestRegressor(\n            n_estimators=300, random_state=random_state, n_jobs=-1\n        )\n\n    n, p = X_train_clean.shape\n    selectors, Xsel_train = {}, {}\n\n    for j, name in enumerate(target_names):\n        yj = y_train[:, j]\n        mask = ~np.isnan(yj)\n        n_lab = int(mask.sum())\n\n        if verbose:\n            print(f\"\\n[{name}] labeled rows = {n_lab} / {n}\")\n\n        if n_lab < 5:\n            if verbose:\n                print(f\"[{name}] Too few labels; skipping selection (keep all {p}).\")\n            # Identity transform: keep all\n            keep_idx = np.arange(p)\n            selectors[name] = keep_idx\n            Xsel_train[name] = X_train_clean\n            continue\n\n        X_known = X_train_clean[mask]\n        y_known = yj[mask]\n\n        # Option A: CV-averaged importances for stability\n        kf = KFold(n_splits=min(cv_splits, n_lab), shuffle=True, random_state=random_state)\n        importances = np.zeros(p, dtype=float)\n        folds = 0\n        for tr_idx, va_idx in kf.split(X_known):\n            try:\n                est = base_estimator.__class__(**base_estimator.get_params())\n                est.fit(X_known[tr_idx], y_known[tr_idx])\n                if hasattr(est, \"feature_importances_\"):\n                    importances += est.feature_importances_\n                    folds += 1\n            except Exception as e:\n                if verbose:\n                    print(f\"[{name}] CV fold failed: {e}\")\n\n        if folds > 0:\n            importances /= folds\n        else:\n            # fallback: single fit\n            est = base_estimator.__class__(**base_estimator.get_params())\n            est.fit(X_known, y_known)\n            importances = getattr(est, \"feature_importances_\", np.ones(p) / p)\n\n        # Decide how aggressive to be\n        if n_lab < min_labels:\n            if verbose:\n                print(f\"[{name}] Using LIGHT selection (few labels).\")\n            # keep top-K or top median by importance\n            if top_k is None:\n                thr = np.median(importances)\n                keep_idx = np.where(importances >= thr)[0]\n            else:\n                keep_idx = np.argsort(importances)[::-1][:top_k]\n        else:\n            if verbose:\n                print(f\"[{name}] Using NORMAL selection.\")\n            if top_k is None:\n                thr = np.median(importances)\n                keep_idx = np.where(importances >= thr)[0]\n            else:\n                keep_idx = np.argsort(importances)[::-1][:top_k]\n\n        if verbose:\n            print(f\"[{name}] kept features: {len(keep_idx)} / {p}\")\n\n        selectors[name] = keep_idx\n        Xsel_train[name] = X_train_clean[:, keep_idx]\n\n    return selectors, Xsel_train\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:27:37.748088Z","iopub.execute_input":"2025-07-30T06:27:37.748796Z","iopub.status.idle":"2025-07-30T06:27:37.758449Z","shell.execute_reply.started":"2025-07-30T06:27:37.748766Z","shell.execute_reply":"2025-07-30T06:27:37.757787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def impute_targets_adaptive(\n    X,                      # np.ndarray (shared features) OR None if using X_per_target\n    y,                      # np.ndarray shape (n_samples, n_targets) with NaNs\n    target_names,           # list[str], e.g., ['Tg','FFV','Tc','Density','Rg']\n    selectors=None,         # optional: dict[name] -> np.ndarray[int] (column indices into X)\n    X_per_target=None,      # optional: dict[name] -> np.ndarray (feature matrix per target)\n    min_missing_for_nn=30,  # use NN only if number of NaNs in that target > this\n    min_known_for_nn=50,    # and at least this many known labels to train the NN\n    epochs=100, batch_size=64, verbose=0,\n    simple_strategy=\"median\",   # 'median' or 'mean'\n    random_state=42\n):\n    \"\"\"\n    Impute missing target values per target.\n    - If missing count > min_missing_for_nn AND known count >= min_known_for_nn -> use a small MLP (Keras).\n    - Else -> use SimpleImputer on the known labels (median/mean).\n    - Supports per-target feature subsets via `selectors` or `X_per_target`.\n    \n    Returns:\n        y_imp       : np.ndarray with missing targets imputed\n        impute_info : dict[name] -> ('nn', model) or ('simple', value or imputer)\n    \"\"\"\n    y_imp = y.copy().astype(float)\n    impute_info = {}\n\n    # Helper to get the design matrix for a given target\n    def get_X_for(name):\n        if X_per_target is not None:\n            return X_per_target[name]\n        if selectors is not None and X is not None:\n            idx = selectors.get(name, None)\n            return X[:, idx] if idx is not None else X\n        # default: shared X\n        return X\n\n    # Build MLP lazily and safely\n    def build_mlp(input_dim):\n        from tensorflow.keras import models, layers, optimizers\n        m = models.Sequential([\n            layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n            layers.Dense(128, activation='relu'),\n            layers.Dense(1)\n        ])\n        m.compile(optimizer=optimizers.Adam(1e-3), loss='mse')\n        return m\n\n    rng = np.random.RandomState(random_state)\n    n_targets = y.shape[1]\n\n    for j, name in enumerate(target_names):\n        y_col = y[:, j]\n        miss_mask = np.isnan(y_col)\n        n_miss = int(miss_mask.sum())\n        known_mask = ~miss_mask\n        n_known = int(known_mask.sum())\n\n        if n_miss == 0:\n            impute_info[name] = ('none', None)\n            # nothing to fill\n            continue\n\n        Xj = get_X_for(name)\n        if Xj is None:\n            # Safety: if we couldn't get features, fall back to simple imputer\n            print(f\"[Impute-{name}] No features provided; using SimpleImputer({simple_strategy}).\")\n            simp = SimpleImputer(strategy=simple_strategy)\n            y_known_2d = y_col[known_mask].reshape(-1, 1)\n            simp.fit(y_known_2d)\n            y_imp[miss_mask, j] = simp.transform(np.full((n_miss, 1), np.nan))[:, 0]\n            impute_info[name] = ('simple', simp)\n            continue\n\n        # Decide method\n        use_nn = (n_miss > min_missing_for_nn) and (n_known >= min_known_for_nn)\n\n        if not use_nn:\n            # Simple fill from known labels statistics\n            try:\n                simp = SimpleImputer(strategy=simple_strategy)\n                y_known_2d = y_col[known_mask].reshape(-1, 1)\n                simp.fit(y_known_2d)\n                y_imp[miss_mask, j] = simp.transform(np.full((n_miss, 1), np.nan))[:, 0]\n                impute_info[name] = ('simple', simp)\n                print(f\"[Impute-{name}] SimpleImputer({simple_strategy}) used | known={n_known}, missing={n_miss}\")\n            except Exception as e:\n                # Final fallback: manual median\n                val = np.nanmedian(y_col)\n                y_imp[miss_mask, j] = val\n                impute_info[name] = ('simple_fallback', val)\n                print(f\"[Impute-{name}] SimpleImputer failed ({e}); filled with median={val:.4f}\")\n            continue\n\n        # NN path\n        try:\n            X_known, y_known = Xj[known_mask], y_col[known_mask]\n            # Small validation split\n            Xtr, Xva, ytr, yva = train_test_split(\n                X_known, y_known, test_size=0.2, random_state=random_state\n            )\n\n            model = build_mlp(Xj.shape[1])\n            model.fit(Xtr, ytr, validation_data=(Xva, yva),\n                      batch_size=batch_size, epochs=epochs, verbose=verbose)\n\n            # Predict missing\n            X_miss = Xj[miss_mask]\n            y_pred = model.predict(X_miss, verbose=0).ravel()\n            y_imp[miss_mask, j] = y_pred\n            impute_info[name] = ('nn', model)\n            print(f\"[Impute-{name}] NN used | known={n_known}, missing={n_miss}\")\n\n        except Exception as e:\n            # Robust fallback: SimpleImputer on labels\n            try:\n                simp = SimpleImputer(strategy=simple_strategy)\n                y_known_2d = y_col[known_mask].reshape(-1, 1)\n                simp.fit(y_known_2d)\n                y_imp[miss_mask, j] = simp.transform(np.full((n_miss, 1), np.nan))[:, 0]\n                impute_info[name] = ('simple_after_nn_fail', simp)\n                print(f\"[Impute-{name}] NN failed ({e}); used SimpleImputer({simple_strategy}).\")\n            except Exception as ee:\n                val = np.nanmedian(y_col)\n                y_imp[miss_mask, j] = val\n                impute_info[name] = ('simple_fallback', val)\n                print(f\"[Impute-{name}] Both NN and SimpleImputer failed ({ee}); median={val:.4f}.\")\n\n    return y_imp, impute_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:27:41.648086Z","iopub.execute_input":"2025-07-30T06:27:41.648677Z","iopub.status.idle":"2025-07-30T06:27:41.662636Z","shell.execute_reply.started":"2025-07-30T06:27:41.648654Z","shell.execute_reply":"2025-07-30T06:27:41.661852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef summarize_targets(y_raw: np.ndarray, y_imp: np.ndarray, target_names):\n    \"\"\"\n    Print descriptive stats of targets before and after imputation safely.\n    \"\"\"\n    try:\n        assert y_raw.shape == y_imp.shape\n        for j, name in enumerate(target_names):\n            print(f\"\\n=== {name} ===\")\n            try:\n                orig = y_raw[:, j]\n                imp  = y_imp[:, j]\n                mask_known   = ~np.isnan(orig)\n                mask_missing =  np.isnan(orig)\n\n                # Known values\n                print(f\"Known (before imputation): n={mask_known.sum()}\")\n                if mask_known.any():\n                    print(pd.Series(orig[mask_known]).describe())\n\n                # Imputed values\n                print(f\"\\nImputed count: {mask_missing.sum()}\")\n                if mask_missing.any():\n                    print(\"Imputed-only stats:\")\n                    print(pd.Series(imp[mask_missing]).describe())\n\n                # Full\n                print(\"\\nAfter imputation (full vector) stats:\")\n                print(pd.Series(imp).describe())\n            except Exception as e:\n                print(f\"Error summarizing {name}: {e}\")\n    except Exception as e:\n        print(f\"safe_summarize_targets() failed: {e}\")\n\n\ndef plot_target_distributions(y_raw: np.ndarray, y_imp: np.ndarray, target_names,\n                                   bins=30, figsize=(10, 4)):\n    \"\"\"\n    Plots distribution of known vs. imputed values per target with error handling.\n    \"\"\"\n    try:\n        assert y_raw.shape == y_imp.shape\n        n_targets = y_raw.shape[1]\n\n        for j, name in enumerate(target_names):\n            try:\n                orig = y_raw[:, j]\n                imp  = y_imp[:, j]\n\n                known_mask   = ~np.isnan(orig)\n                missing_mask =  np.isnan(orig)\n\n                fig, ax = plt.subplots(1, 2, figsize=figsize, constrained_layout=True)\n                fig.suptitle(f\"{name} — distributions before & after imputation\", fontsize=12)\n\n                if known_mask.any():\n                    ax[0].hist(orig[known_mask], bins=bins)\n                ax[0].set_title(f\"{name} (known only)\\n n={known_mask.sum()}\")\n                ax[0].set_xlabel(name)\n                ax[0].set_ylabel(\"Count\")\n\n                if missing_mask.any():\n                    ax[1].hist(imp[missing_mask], bins=bins)\n                ax[1].set_title(f\"{name} (imputed only)\\n n={missing_mask.sum()}\")\n                ax[1].set_xlabel(name)\n                ax[1].set_ylabel(\"Count\")\n\n                plt.show()\n\n            except Exception as e:\n                print(f\"Error plotting {name}: {e}\")\n    except Exception as e:\n        print(f\"safe_plot_target_distributions() failed: {e}\")\n\n\ndef plot_overlay_pre_post(y_raw: np.ndarray, y_imp: np.ndarray, target_names,\n                               bins=30, alpha_known=0.6, alpha_imp=0.6, figsize=(6,4)):\n    \"\"\"\n    Overlay known vs. imputed distributions for each target.\n    \"\"\"\n    try:\n        for j, name in enumerate(target_names):\n            try:\n                orig = y_raw[:, j]\n                imp  = y_imp[:, j]\n                known_mask   = ~np.isnan(orig)\n                missing_mask =  np.isnan(orig)\n\n                plt.figure(figsize=figsize)\n                if known_mask.any():\n                    plt.hist(orig[known_mask], bins=bins, alpha=alpha_known,\n                             label=\"Known (pre)\", density=True)\n                if missing_mask.any():\n                    plt.hist(imp[missing_mask], bins=bins, alpha=alpha_imp,\n                             label=\"Imputed-only (post)\", density=True)\n                plt.title(f\"{name} — overlay distributions\")\n                plt.xlabel(name)\n                plt.ylabel(\"Density\")\n                plt.legend()\n                plt.show()\n\n            except Exception as e:\n                print(f\"Error in overlay plot for {name}: {e}\")\n    except Exception as e:\n        print(f\"safe_plot_overlay_pre_post() failed: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:47:02.044737Z","iopub.execute_input":"2025-07-30T06:47:02.045054Z","iopub.status.idle":"2025-07-30T06:47:02.058100Z","shell.execute_reply.started":"2025-07-30T06:47:02.045034Z","shell.execute_reply":"2025-07-30T06:47:02.057417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _make_model_specs(random_state: int = 42):\n    specs = []\n\n    if _HAS_LGBM:\n        specs.append((\n            \"lgbm\",\n            lambda: LGBMRegressor(\n                n_estimators=1200, learning_rate=0.03, num_leaves=63,\n                subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n                random_state=random_state, n_jobs=-1\n            )\n        ))\n\n    if _HAS_XGB:\n        specs.append((\n            \"xgb\",\n            lambda: XGBRegressor(\n                n_estimators=1200, learning_rate=0.03, max_depth=8,\n                subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n                objective=\"reg:squarederror\", tree_method=\"hist\",\n                random_state=random_state, n_jobs=-1\n            )\n        ))\n\n    specs.append((\n        \"rf\",\n        lambda: RandomForestRegressor(\n            n_estimators=800, max_features=\"sqrt\",\n            random_state=random_state, n_jobs=-1\n        )\n    ))\n\n    specs.append((\n        \"et\",\n        lambda: ExtraTreesRegressor(\n            n_estimators=1000, max_features=\"sqrt\",\n            random_state=random_state, n_jobs=-1\n        )\n    ))\n\n    specs.append((\n        \"enet\",\n        lambda: ElasticNet(alpha=1e-3, l1_ratio=0.2,\n                           random_state=random_state, max_iter=3000)\n    ))\n\n    return specs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T07:35:03.288535Z","iopub.execute_input":"2025-07-30T07:35:03.289012Z","iopub.status.idle":"2025-07-30T07:35:03.295150Z","shell.execute_reply.started":"2025-07-30T07:35:03.288989Z","shell.execute_reply":"2025-07-30T07:35:03.294424Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Building feature matrix","metadata":{}},{"cell_type":"code","source":"X_train_raw, desc_names, fp_names = build_features(train)\nX_test_raw, _, _ = build_features(test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:27:58.930286Z","iopub.execute_input":"2025-07-30T06:27:58.930837Z","iopub.status.idle":"2025-07-30T06:30:18.114950Z","shell.execute_reply.started":"2025-07-30T06:27:58.930814Z","shell.execute_reply":"2025-07-30T06:30:18.114129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = train.loc[X_train_raw.index]\ntest  = test.loc[X_test_raw.index]\n\ny_train_raw = train[TARGETS].to_numpy(dtype=float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:30:35.156182Z","iopub.execute_input":"2025-07-30T06:30:35.156764Z","iopub.status.idle":"2025-07-30T06:30:35.167583Z","shell.execute_reply.started":"2025-07-30T06:30:35.156740Z","shell.execute_reply":"2025-07-30T06:30:35.167123Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"X_train_clean, state, info = fit_preprocessor(X_train_raw, variance_threshold=0.0, corr_threshold=0.90, scale=True)\nprint(\"Dropped (NaN):\", len(info['dropped_nan_cols']))\nprint(\"Dropped (low var):\", len(info['dropped_lowvar_cols']))\nprint(\"Dropped (corr):\", len(info['dropped_corr_cols']))\nprint(\"Retained:\", len(info['retained_cols']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:30:37.314371Z","iopub.execute_input":"2025-07-30T06:30:37.315066Z","iopub.status.idle":"2025-07-30T06:30:46.701841Z","shell.execute_reply.started":"2025-07-30T06:30:37.315040Z","shell.execute_reply":"2025-07-30T06:30:46.701083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"report_nan_columns_numpy(X_train_clean, name=\"X_train_clean\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:30:53.519230Z","iopub.execute_input":"2025-07-30T06:30:53.519689Z","iopub.status.idle":"2025-07-30T06:30:53.536271Z","shell.execute_reply.started":"2025-07-30T06:30:53.519666Z","shell.execute_reply":"2025-07-30T06:30:53.535563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test_clean = transform_preprocessor(X_test_raw, state)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:30:55.910252Z","iopub.execute_input":"2025-07-30T06:30:55.910509Z","iopub.status.idle":"2025-07-30T06:30:55.924856Z","shell.execute_reply.started":"2025-07-30T06:30:55.910490Z","shell.execute_reply":"2025-07-30T06:30:55.924257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Supervised Feature Selection","metadata":{}},{"cell_type":"code","source":"selectors, Xsel_train = per_target_supervised_selection(\n    X_train_clean, y_train_raw, TARGETS,\n    min_labels=500,           # light selection when labels are ~500–700\n    top_k=100,                # e.g., cap to 100 features per target\n    cv_splits=5, verbose=True\n)\n\n# Applying the same masks to test\nXsel_test = {name: X_test_clean[:, idx] for name, idx in selectors.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:30:59.484313Z","iopub.execute_input":"2025-07-30T06:30:59.484960Z","iopub.status.idle":"2025-07-30T06:34:55.202295Z","shell.execute_reply.started":"2025-07-30T06:30:59.484940Z","shell.execute_reply":"2025-07-30T06:34:55.201595Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Target Data Imputation","metadata":{}},{"cell_type":"code","source":"y_imputed, impute_info = impute_targets_adaptive(\n    X=None, y=y_train_raw, target_names=TARGETS,\n    X_per_target=Xsel_train,            # directly pass per‑target feature matrices\n    min_missing_for_nn=30, min_known_for_nn=50\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:35:02.667964Z","iopub.execute_input":"2025-07-30T06:35:02.668521Z","iopub.status.idle":"2025-07-30T06:36:28.594717Z","shell.execute_reply.started":"2025-07-30T06:35:02.668499Z","shell.execute_reply":"2025-07-30T06:36:28.593860Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plotting target distributions before and after imputation","metadata":{}},{"cell_type":"code","source":"summarize_targets(y_train_raw, y_imputed, TARGETS)\nplot_target_distributions(y_train_raw, y_imputed, TARGETS, bins=40)\nplot_overlay_pre_post(y_train_raw, y_imputed, TARGETS, bins=40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T06:47:41.689200Z","iopub.execute_input":"2025-07-30T06:47:41.689482Z","iopub.status.idle":"2025-07-30T06:47:45.586364Z","shell.execute_reply.started":"2025-07-30T06:47:41.689462Z","shell.execute_reply":"2025-07-30T06:47:45.585719Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training ensemble models","metadata":{}},{"cell_type":"code","source":"def train_and_eval_mae(Xsel_train, y_imputed, target_names, valid_size=0.2, random_state=42):\n    \"\"\"\n    Split train/validation once, train multiple models per target, compute MAE per target\n    and the unweighted mean MAE across targets.\n    \"\"\"\n    n, K = y_imputed.shape\n    idx_all = np.arange(n)\n    train_idx, valid_idx = train_test_split(\n        idx_all, test_size=valid_size, random_state=random_state, shuffle=True\n    )\n\n    results, fitted, preds = {}, {}, {}\n    specs = _make_model_specs(random_state)\n\n    for mname, builder in specs:\n        results[mname] = {\"per_target\": {}, \"mean_mae\": None}\n        fitted[mname] = {}\n        preds[mname] = {\"val\": np.full((len(valid_idx), K), np.nan)}\n        print(f\"\\n===== {mname} =====\")\n\n        maes = []\n        for j, tname in enumerate(target_names):\n            try:\n                X = Xsel_train[tname]\n                y = y_imputed[:, j]\n\n                X_tr, X_va = X[train_idx], X[valid_idx]\n                y_tr, y_va = y[train_idx], y[valid_idx]\n\n                model = builder()\n\n                if mname == \"enet\":\n                    scaler = StandardScaler()\n                    X_tr_s = scaler.fit_transform(X_tr)\n                    X_va_s = scaler.transform(X_va)\n                    model.fit(X_tr_s, y_tr)\n                    y_hat = model.predict(X_va_s)\n                    fitted[mname][tname] = (scaler, model)\n                else:\n                    model.fit(X_tr, y_tr)\n                    y_hat = model.predict(X_va)\n                    fitted[mname][tname] = model\n\n                preds[mname][\"val\"][:, j] = y_hat\n                mae = mean_absolute_error(y_va, y_hat)\n                results[mname][\"per_target\"][tname] = float(mae)\n                maes.append(mae)\n                print(f\"{tname}: MAE={mae:.4f}\")\n\n            except Exception as e:\n                print(f\"[{mname}::{tname}] failed: {e}\")\n                results[mname][\"per_target\"][tname] = np.nan\n\n        mean_mae = np.nanmean(list(results[mname][\"per_target\"].values()))\n        results[mname][\"mean_mae\"] = mean_mae\n        print(f\"Mean MAE across targets: {mean_mae:.4f}\")\n\n    split = {\"train_idx\": train_idx, \"valid_idx\": valid_idx}\n    return results, fitted, preds, split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T07:40:04.601879Z","iopub.execute_input":"2025-07-30T07:40:04.602469Z","iopub.status.idle":"2025-07-30T07:40:04.611813Z","shell.execute_reply.started":"2025-07-30T07:40:04.602443Z","shell.execute_reply":"2025-07-30T07:40:04.611052Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results, fitted, preds, split = train_and_eval_mae(\n    Xsel_train=Xsel_train,\n    y_imputed=y_imputed,\n    target_names=TARGETS\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T07:40:31.549513Z","iopub.execute_input":"2025-07-30T07:40:31.549788Z","iopub.status.idle":"2025-07-30T07:43:14.476131Z","shell.execute_reply.started":"2025-07-30T07:40:31.549768Z","shell.execute_reply":"2025-07-30T07:43:14.474933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Retraining on the LightGBM model","metadata":{}},{"cell_type":"code","source":"def build_lgbm(random_state=42):\n    return LGBMRegressor(\n        n_estimators=1200,\n        learning_rate=0.03,\n        num_leaves=63,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        reg_lambda=1.0,\n        random_state=random_state,\n        n_jobs=-1\n    )\nmodels_lgbm = {}\nn_test = next(iter(Xsel_test.values())).shape[0]\ntest_pred = np.zeros((n_test, len(TARGETS)), dtype=float)\n\nfor j, tname in enumerate(TARGETS):\n    X_tr = Xsel_train[tname]\n    y_tr = y_imputed[:, j]\n    X_te = Xsel_test[tname]\n\n    model = build_lgbm()\n    model.fit(X_tr, y_tr)\n    models_lgbm[tname] = model\n\n    test_pred[:, j] = model.predict(X_te)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T07:58:55.837639Z","iopub.execute_input":"2025-07-30T07:58:55.838408Z","iopub.status.idle":"2025-07-30T07:59:31.298902Z","shell.execute_reply.started":"2025-07-30T07:58:55.838382Z","shell.execute_reply":"2025-07-30T07:59:31.298331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sub = pd.DataFrame(test_pred, columns=TARGETS)\nsub.insert(0, 'id', test['id'].values)\nsub.to_csv('submission.csv', index=False)\nprint(\"Saved submission.csv:\", sub.shape)\ndisplay(sub.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-30T07:59:56.795403Z","iopub.execute_input":"2025-07-30T07:59:56.795657Z","iopub.status.idle":"2025-07-30T07:59:56.812752Z","shell.execute_reply.started":"2025-07-30T07:59:56.795638Z","shell.execute_reply":"2025-07-30T07:59:56.812049Z"}},"outputs":[],"execution_count":null}]}